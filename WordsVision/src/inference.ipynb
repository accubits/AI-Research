{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanSolo/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: 'werkzeug.contrib.cache' is deprecated as of version 0.15 and will be removed in version 1.0. It has moved to https://github.com/pallets/cachelib.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from werkzeug.contrib.cache import SimpleCache\n",
    "cache = SimpleCache()\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import cv2\n",
    "\n",
    "import model\n",
    "from model import RNN_ENCODER, G_NET, CNN_ENCODER, G_DCGAN, D_NET256\n",
    "from miscc.utils import weights_init\n",
    "from datasets import TextDataset, prepare_data\n",
    "from miscc.utils import build_super_images, build_super_images2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanSolo/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netG = G_NET()\n",
    "netG.apply(weights_init)\n",
    "netG.cuda()\n",
    "netG.eval()\n",
    "#\n",
    "text_encoder = RNN_ENCODER(27297, nhidden=256)\n",
    "state_dict = \\\n",
    "    torch.load('../models/text_encoder100.pth', map_location=lambda storage, loc: storage)\n",
    "text_encoder.load_state_dict(state_dict)\n",
    "text_encoder = text_encoder.cuda()\n",
    "text_encoder.eval()\n",
    "\n",
    "state_dict = \\\n",
    "    torch.load('../models/coco_AttnGAN2.pth', map_location=lambda storage, loc: storage)\n",
    "netG.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load filenames from: ../data/coco//train/filenames.pickle (82783)\n",
      "Load filenames from: ../data/coco//test/filenames.pickle (40470)\n",
      "Load from:  ../data/coco/captions.pickle\n"
     ]
    }
   ],
   "source": [
    "imsize = 64 * (2 ** (3 - 1))\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize(int(imsize * 76 / 64)),\n",
    "    transforms.RandomCrop(imsize),\n",
    "    transforms.RandomHorizontalFlip()])\n",
    "\n",
    "dataset = TextDataset('../data/coco/', 'test',\n",
    "                      base_size=64,\n",
    "                      transform=image_transform)\n",
    "assert dataset\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=batch_size,\n",
    "    drop_last=True, shuffle=True, num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordtoix = dataset.wordtoix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_img():\n",
    "    output=[]\n",
    "    '''generate images from example sentences'''\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    data_dic = {}\n",
    "\n",
    "\n",
    "    sentences = ['There is a sun in the middle of the sky']\n",
    "    # a list of indices for a sentence\n",
    "    captions = []\n",
    "    cap_lens = []\n",
    "    for sent in sentences:\n",
    "        if len(sent) == 0:\n",
    "            continue\n",
    "        sent = sent.replace(\"\\ufffd\\ufffd\", \" \")\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        tokens = tokenizer.tokenize(sent.lower())\n",
    "        if len(tokens) == 0:\n",
    "            print('sent', sent)\n",
    "            continue\n",
    "\n",
    "        rev = []\n",
    "        for t in tokens:\n",
    "            t = t.encode('ascii', 'ignore').decode('ascii')\n",
    "            if len(t) > 0 and t in wordtoix:\n",
    "                rev.append(wordtoix[t])\n",
    "        captions.append(rev)\n",
    "        cap_lens.append(len(rev))\n",
    "    max_len = np.max(cap_lens)\n",
    "\n",
    "    sorted_indices = np.argsort(cap_lens)[::-1]\n",
    "    cap_lens = np.asarray(cap_lens)\n",
    "    cap_lens = cap_lens[sorted_indices]\n",
    "    cap_array = np.zeros((len(captions), max_len), dtype='int64')\n",
    "    for i in range(len(captions)):\n",
    "        idx = sorted_indices[i]\n",
    "        cap = captions[idx]\n",
    "        c_len = len(cap)\n",
    "        cap_array[i, :c_len] = cap\n",
    "    data_dic[0] = [cap_array, cap_lens, sorted_indices]\n",
    "    for key in data_dic:\n",
    "        save_dir = 'op/'\n",
    "        captions, cap_lens, sorted_indices = data_dic[key]\n",
    "\n",
    "        batch_size = captions.shape[0]\n",
    "        nz = 100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            captions = Variable(torch.from_numpy(captions))\n",
    "            cap_lens = Variable(torch.from_numpy(cap_lens))\n",
    "\n",
    "            captions = captions.cuda()\n",
    "            cap_lens = cap_lens.cuda()\n",
    "\n",
    "        for i in range(1):  # 16\n",
    "            with torch.no_grad():\n",
    "                noise = Variable(torch.FloatTensor(batch_size, nz))\n",
    "                noise = noise.cuda()\n",
    "            #######################################################\n",
    "            # (1) Extract text embeddings\n",
    "            ######################################################\n",
    "            hidden = text_encoder.init_hidden(batch_size)\n",
    "            # words_embs: batch_size x nef x seq_len\n",
    "            # sent_emb: batch_size x nef\n",
    "            words_embs, sent_emb = text_encoder(captions, cap_lens, hidden)\n",
    "            mask = (captions == 0)\n",
    "            #######################################################\n",
    "            # (2) Generate fake images\n",
    "            ######################################################\n",
    "            noise.data.normal_(0, 1)\n",
    "            fake_imgs, attention_maps, _, _ = netG(noise, sent_emb, words_embs, mask)\n",
    "            # G attention\n",
    "            cap_lens_np = cap_lens.cpu().data.numpy()\n",
    "            for j in range(batch_size):\n",
    "                save_name = '%s/%d_s_%d' % (save_dir, i, sorted_indices[j])\n",
    "                for k in range(len(fake_imgs)):\n",
    "                    im = fake_imgs[k][j].data.cpu().numpy()\n",
    "                    im = (im + 1.0) * 127.5\n",
    "                    im = im.astype(np.uint8)\n",
    "                    # print('im', im.shape)\n",
    "                    im = np.transpose(im, (1, 2, 0))\n",
    "                    # print('im', im.shape)\n",
    "                    im = Image.fromarray(im)\n",
    "                    fullpath = '%s_g%d.png' % (save_name, k)\n",
    "                    output.append(im)\n",
    "\n",
    "                for k in range(len(attention_maps)):\n",
    "                    if len(fake_imgs) > 1:\n",
    "                        im = fake_imgs[k + 1].detach().cpu()\n",
    "                    else:\n",
    "                        im = fake_imgs[0].detach().cpu()\n",
    "                    attn_maps = attention_maps[k]\n",
    "                    att_sze = attn_maps.size(2)\n",
    "                    img_set, sentences = \\\n",
    "                        build_super_images2(im[j].unsqueeze(0),\n",
    "                                            captions[j].unsqueeze(0),\n",
    "                                            [cap_lens_np[j]], dataset.ixtoword,\n",
    "                                            [attn_maps[j]], att_sze)\n",
    "                    if img_set is not None:\n",
    "                        im = Image.fromarray(img_set)\n",
    "#                         fullpath = '%s_a%d.png' % (save_name, k)\n",
    "                        output.append(im)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/D4567C46567C2AFE/Work HDD/Research/AI-Research/Words-Vision-AttnGAN/src/miscc/utils.py:239: RuntimeWarning: invalid value encountered in true_divide\n",
      "  one_map = (one_map - minV) / (maxV - minV)\n"
     ]
    }
   ],
   "source": [
    "output = gen_img()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.Image.Image"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
